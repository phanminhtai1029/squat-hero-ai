# üßò Yoga Pose AI - Real-time Pose Recognition & Analysis

<p align="center">
  <strong>AI-Powered Yoga Pose Recognition using GNN & Vector Similarity</strong>
</p>

<p align="center">
  <a href="#features">Features</a> ‚Ä¢
  <a href="#architecture">Architecture</a> ‚Ä¢
  <a href="#installation">Installation</a> ‚Ä¢
  <a href="#usage">Usage</a> ‚Ä¢
  <a href="#benchmarks">Benchmarks</a> ‚Ä¢
  <a href="#dataset">Dataset</a>
</p>

---

## üìñ Overview

**Yoga Pose AI** is a real-time computer vision system that recognizes and analyzes yoga poses using advanced pose estimation and Graph Neural Networks (GNN). Built with PyTorch and YOLOv8, it supports **115 yoga poses** with high accuracy and real-time performance.

### Key Technologies
- üéØ **YOLOv8-Pose** - Fast pose detection (CUDA-accelerated)
- üß† **SimplePoseGNN** - 320K parameter encoder for pose embeddings
- üîç **FAISS** - Fast vector similarity search (CPU/GPU)
- üìä **OpenCV** - Real-time video processing

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| üßò **115 Yoga Poses** | Support for 115+ poses from multiple datasets |
| ‚ö° **Real-time** | 597 FPS vector matching, 937 FPS angle matching |
| üéØ **Dual Methods** | Angle-based (hand-crafted) + GNN-based (learned) |
| üìà **Benchmarked** | Complete baseline metrics on 1,468 test images |
| üî¢ **Vector Database** | 1,110 pose embeddings with FAISS indexing |
| üé¨ **Video Pipeline** | Frame capture ‚Üí Person detection ‚Üí Pose estimation ‚Üí Matching |

---

## üîÑ Pipeline Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     YOGA POSE AI PIPELINE (6 STEPS)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   STEP 1    ‚îÇ  ‚îÇ   STEP 2    ‚îÇ  ‚îÇ   STEP 3    ‚îÇ  ‚îÇ   STEP 4    ‚îÇ ‚îÇ
‚îÇ  ‚îÇFrame Capture‚îÇ‚îÄ‚ñ∂‚îÇPerson Detect‚îÇ‚îÄ‚ñ∂‚îÇ Pose Est.   ‚îÇ‚îÄ‚ñ∂‚îÇ Classifier  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (OpenCV)   ‚îÇ  ‚îÇ  (YOLOv8)   ‚îÇ  ‚îÇ(YOLOv8-Pose)‚îÇ  ‚îÇ   (GNN)     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                ‚îÇ          ‚îÇ
‚îÇ         ‚ñº                ‚ñº                ‚ñº                ‚ñº          ‚îÇ
‚îÇ    Video Frame    Person BBox      17 Keypoints     128D Embedding   ‚îÇ
‚îÇ                                     (COCO format)                     ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ  ‚îÇ   STEP 5    ‚îÇ  ‚îÇ   STEP 6    ‚îÇ                                    ‚îÇ
‚îÇ  ‚îÇ  Matcher    ‚îÇ‚îÄ‚ñ∂‚îÇVisualization‚îÇ                                    ‚îÇ
‚îÇ  ‚îÇ(FAISS/Angle)‚îÇ  ‚îÇ  (OpenCV)   ‚îÇ                                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                                            ‚îÇ
‚îÇ         ‚ñº                ‚ñº                                            ‚îÇ
‚îÇ   Pose Name        Annotated Frame                                   ‚îÇ
‚îÇ   + Similarity      + Pose Label                                     ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Module Breakdown

| Module | File | Purpose |
|--------|------|---------|
| **Step 1** | `step1_frame_capture.py` | Capture frames from webcam/video |
| **Step 2** | `step2_person_detection.py` | YOLOv8 person detection & cropping |
| **Step 3** | `step3_pose_estimation.py` | Extract 17 COCO keypoints |
| **Step 4** | `step4_frame_classifier.py` | GNN encoder (128D embeddings) |
| **Step 5a** | `step5a_angle_matcher.py` | Angle-based matching (8 joint angles) |
| **Step 5b** | `step5b_vector_matcher.py` | FAISS vector similarity search |
| **Utils** | `angle_calculator.py` | Joint angle computation |

---

## üöÄ Installation

### Prerequisites
- Python 3.12+
- CUDA 11.8+ (optional, for GPU acceleration)
- 8GB+ RAM

### Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/squat-hero-ai.git
cd squat-hero-ai

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Download YOLOv8 model
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt

# Optional: Install FAISS for faster matching
pip install faiss-cpu  # CPU only
# pip install faiss-gpu  # GPU support
```

---

## üìä Usage

### 1. Build Vector Database

```bash
# Build pose embeddings from training data (115 poses)
python ml/build_vector_database_simple.py

# Output: data/pose_vectors_final.npz (1,110 samples, 128D, 513KB)
```

### 2. Run Baseline Benchmark

```bash
# Test both angle-based and vector-based methods
python ml/baseline_benchmark.py

# Results saved to: evaluation/baseline_benchmark.json
```

### 3. Real-time Pose Recognition (Coming Soon)

```bash
# Run with webcam
python main.py --mode realtime

# Run on video file
python main.py --mode video --input path/to/video.mp4
```

---

## üìà Benchmarks

### Baseline Performance (115 Poses, 1,468 Test Images)

| Method | Accuracy | Precision | Recall | F1 | Speed |
|--------|----------|-----------|--------|-----|-------|
| **Angle-based** | **9.18%** | 10.56% | 9.18% | 8.18% | **937 FPS** (1.07ms) |
| **Vector-based (untrained GNN)** | **0.92%** | 0.01% | 0.92% | 0.03% | **597 FPS** (1.67ms) |

**Top 5 Classes (Angle-based):**
1. Durvasasana: 71.4%
2. Dandasana: 55.6%
3. Trikonasana: 50.0%
4. Virabhadrasana III: 50.0%
5. Prasarita Padottanasana: 36.4%

**Hardware:** RTX 3060 Laptop 6GB, CUDA 13.0, Ubuntu 22.04

**Notes:**
- Angle method: Hand-crafted features, no training needed
- Vector method: Random GNN weights (baseline before training)
- After GNN training, expected >20% accuracy

---

## üì¶ Dataset

### Merged Dataset (115 Poses, 9,125 Images)

**Sources:**
1. **Yoga_Poses-Dataset** - 8 poses, 484 images
2. **Yoga Posture Dataset** (Kaggle) - 47 poses, 2,756 images
3. **Yoga 107 Poses** (Kaggle) - 107 poses, 5,991 images

**Split:**
- Train: 6,342 images (70%)
- Validation: 1,315 images (15%)
- Test: 1,468 images (15%)

**Download Datasets:**

```bash
# Kaggle datasets (requires Kaggle API credentials)
kaggle datasets download -d tr1gg3rtrash/yoga-posture-dataset
kaggle datasets download -d ujjwalchowdhury/yoga-pose-classification

# Extract to data/
unzip yoga-posture-dataset.zip -d data/
unzip yoga-pose-classification.zip -d data/
```

**Build Final Dataset:**

```bash
# Merge all 3 datasets, normalize pose names, filter <20 samples
python ml/merge_all_datasets.py

# Verify merge correctness
python ml/verify_merge.py

# Analyze dataset sufficiency
python ml/analyze_dataset_sufficiency.py
```

---

## üß† Model Architecture

### SimplePoseGNN (320K Parameters)

```
Input: (17, 2) keypoints [x, y]
  ‚Üì
Edge Index: Skeleton connectivity (COCO format)
  ‚Üì
GraphConv1: 2 ‚Üí 64 channels + ReLU + Dropout(0.2)
  ‚Üì
GraphConv2: 64 ‚Üí 128 channels + ReLU + Dropout(0.2)
  ‚Üì
GraphConv3: 128 ‚Üí 256 channels + ReLU
  ‚Üì
Global Mean Pooling: (17, 256) ‚Üí (256,)
  ‚Üì
Linear: 256 ‚Üí 128D embedding
  ‚Üì
L2 Normalize
  ‚Üì
Output: 128D vector for FAISS matching
```

**Training Config (Planned):**
- Optimizer: AdamW (lr=1e-3, weight_decay=1e-4)
- Loss: TripletMarginLoss (margin=0.2)
- Batch size: 32
- Epochs: 50
- Data augmentation: Random rotation, scaling, noise

---

## üìÅ Project Structure

```
squat-hero-ai/
‚îú‚îÄ‚îÄ pipeline/               # 6-step modular pipeline
‚îÇ   ‚îú‚îÄ‚îÄ step1_frame_capture.py
‚îÇ   ‚îú‚îÄ‚îÄ step2_person_detection.py
‚îÇ   ‚îú‚îÄ‚îÄ step3_pose_estimation.py
‚îÇ   ‚îú‚îÄ‚îÄ step4_frame_classifier.py
‚îÇ   ‚îú‚îÄ‚îÄ step5a_angle_matcher.py
‚îÇ   ‚îî‚îÄ‚îÄ step5b_vector_matcher.py
‚îú‚îÄ‚îÄ ml/                     # Training & benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pose_gnn_encoder.py        # SimplePoseGNN
‚îÇ   ‚îú‚îÄ‚îÄ baseline_benchmark.py          # Full benchmark
‚îÇ   ‚îú‚îÄ‚îÄ baseline_benchmark_fast.py     # Fast 200-sample test
‚îÇ   ‚îú‚îÄ‚îÄ build_vector_database_simple.py
‚îÇ   ‚îú‚îÄ‚îÄ merge_all_datasets.py
‚îÇ   ‚îú‚îÄ‚îÄ verify_merge.py
‚îÇ   ‚îî‚îÄ‚îÄ analyze_dataset_sufficiency.py
‚îú‚îÄ‚îÄ utils/                  # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ angle_calculator.py
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py
‚îú‚îÄ‚îÄ data/                   # Datasets & vectors
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yoga_final/     # 115 poses, train/val/test
‚îÇ   ‚îú‚îÄ‚îÄ pose_database.yaml
‚îÇ   ‚îî‚îÄ‚îÄ pose_vectors_final.npz  # 1,110 embeddings
‚îú‚îÄ‚îÄ evaluation/             # Benchmark results
‚îÇ   ‚îú‚îÄ‚îÄ baseline_benchmark.json
‚îÇ   ‚îú‚îÄ‚îÄ comparison_results.json
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_pipeline.py
‚îú‚îÄ‚îÄ docs/                   # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ IMPLEMENTATION_PLAN_MVP.md
‚îÇ   ‚îî‚îÄ‚îÄ YOGA_POSE_AI_PIPELINE.md
‚îú‚îÄ‚îÄ config.py               # Configuration
‚îú‚îÄ‚îÄ main.py                 # Entry point
‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
‚îî‚îÄ‚îÄ README.md               # This file
```

---

## ‚öôÔ∏è Configuration

Edit `config.py` for custom settings:

```python
# Pose Detection
POSE_MODEL = 'yolov8s-pose.pt'  # or yolov8m-pose.pt (more accurate)
POSE_CONFIDENCE = 0.3

# Person Detection  
PERSON_CONFIDENCE = 0.5
PERSON_IOU = 0.5

# Vector Matching
VECTOR_DATABASE = 'data/pose_vectors_final.npz'
TOP_K_MATCHES = 5
MIN_SIMILARITY = 0.15

# Angle Matching
ANGLE_JOINTS = ['elbow', 'shoulder', 'hip', 'knee']  # 8 angles total
```

---

## üéØ Next Steps

- [ ] Train GNN encoder on 6,342 training images
- [ ] Target: >20% accuracy (vs 9.18% angle baseline)
- [ ] Implement data augmentation
- [ ] Add real-time webcam interface
- [ ] Export models to ONNX for deployment
- [ ] Build web/mobile demo

---

## üìù License

MIT License - see [LICENSE](LICENSE) file

---

## üôè Acknowledgments

- **Datasets**: Yoga_Poses-Dataset, Kaggle Yoga datasets
- **Models**: Ultralytics YOLOv8, PyTorch Geometric
- **Libraries**: OpenCV, FAISS, NumPy

---

## üìß Contact

For questions or collaboration: [your-email@example.com]

**GitHub**: [yourusername/squat-hero-ai](https://github.com/yourusername/squat-hero-ai)

---

<p align="center">Made with ‚ù§Ô∏è for the yoga community</p>
